{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zyusc/Efficient_Fine_Tuning_LLM_on_Single_GPU/blob/main/%E2%80%9CEE599_ML_Systems_HW6_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxH3R-8OZOpk"
      },
      "source": [
        "# HW6 EE599 Systems for Machine Learning, Fall 2023\n",
        "University of Southern California\n",
        "\n",
        "Instructors: Arash Saifhashemi, Murali Annavaram\n",
        "\n",
        "In this homework assignment, we're going to implement DP-SGD for neural networks! Recall from the class that classical ML models possessed convenient properties such as Convexity and L-Lipschitzness which make the DP analysis easy. However, modern neural networks, like the one we'll be working with, does not have these properties. Thus, we need to modify the training algorithm, gradient descent, so that the trained model is DP.\n",
        "\n",
        "## Prerequisites:\n",
        "\n",
        "Set the runtime type to GPU. (Runtime -> Change Runtime Type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQQk25oQCo8f"
      },
      "source": [
        "## Mount Google Drive\n",
        "\n",
        "Create a folder named `HW6` under `ML_Systems` in your Google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ezmjLOr6Co8f",
        "outputId": "4ad79c98-231a-4731-a0d5-c2cce141723c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import sys, os\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/m3_11_26\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1ODkkuwZTSD"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "This section imports all required packages from PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnT0mf2l0hUC",
        "outputId": "3338d441-7fde-4f88-95b4-d0bab45bf80a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7807fe06e930>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B5DlxLjCo8g"
      },
      "source": [
        "**Reminder:** set the runtime type to \"GPU\", or your code will run much more slowly on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22wBDD60Co8h"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQA2aMuNbv48"
      },
      "source": [
        "Define model architecture and prepare dataset, which are the same as the previous homework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km-kc-wa0n5d"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Define the layers of the neural network architecture\n",
        "\n",
        "        # First convolutional layer: 3 input channels, 6 output channels, kernel size 5x5\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5, bias=False)\n",
        "\n",
        "        # Max pooling layer with kernel size 2x2 and stride 2\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Second convolutional layer: 6 input channels, 16 output channels, kernel size 5x5\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5, bias=False)\n",
        "\n",
        "        # Fully connected (dense) layers\n",
        "\n",
        "        # First fully connected layer: 16*5*5 input features, 120 output features\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120, bias=False)\n",
        "\n",
        "        # Second fully connected layer: 120 input features, 84 output features\n",
        "        self.fc2 = nn.Linear(120, 84, bias=False)\n",
        "\n",
        "        # Third fully connected layer: 84 input features, 10 output features (for classification)\n",
        "        self.fc3 = nn.Linear(84, 10, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Forward pass through the neural network\n",
        "\n",
        "        # Apply first convolutional layer, followed by ReLU activation and max pooling\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "\n",
        "        # Apply second convolutional layer, followed by ReLU activation and max pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        # The output from the convolutional and pooling layers is in the form of a\n",
        "        # 3D tensor (height, width, depth or channels).\n",
        "        # To feed this tensor into a fully connected layer,\n",
        "        # it needs to be flattened into a 1D tensor.\n",
        "        # Reshape tensor for fully connected layers\n",
        "        # A2D tensor with a shape of [batch_size, 16 * 5 * 5].\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "\n",
        "        # Apply first fully connected layer, followed by ReLU activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Apply second fully connected layer, followed by ReLU activation\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Apply third fully connected layer (output layer)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heygH3k7ZI0L"
      },
      "outputs": [],
      "source": [
        "# A function to evaluate the performance of a given neural network model\n",
        "# using a test dataset.\n",
        "# It calculates the accuracy of the model's predictions on the test data.\n",
        "def calculate_accuracy(\n",
        "    model: nn.Module, dataloader: DataLoader, max_samples=None\n",
        ") -> float:\n",
        "    correct_predictions = 0  # Initialize the count of correctly predicted samples\n",
        "    total_samples = 0  # Initialize the count of total samples\n",
        "    inference_count = 0  # Initialize the count of inferences made\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        for batch_data in dataloader:\n",
        "            images, labels = batch_data  # Separate images and labels from the batch\n",
        "\n",
        "            images = images.to(device)  # Move images to the specified device\n",
        "            labels = labels.to(device)  # Move labels to the specified device\n",
        "\n",
        "            outputs = model(images)  # Forward pass to get model predictions\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get predicted class indices\n",
        "\n",
        "            total_samples += labels.size(0)  # Increment the total count of samples\n",
        "            correct_predictions += (\n",
        "                (predicted == labels).sum().item()\n",
        "            )  # Count correct predictions\n",
        "\n",
        "            if (\n",
        "                max_samples\n",
        "            ):  # Check if a maximum number of samples for testing is specified\n",
        "                inference_count += images.shape[\n",
        "                    0\n",
        "                ]  # Increment the count of inferences made\n",
        "                if (\n",
        "                    inference_count > max_samples\n",
        "                ):  # Stop testing if maximum samples reached\n",
        "                    break\n",
        "\n",
        "    accuracy = (\n",
        "        100 * correct_predictions / total_samples\n",
        "    )  # Calculate the accuracy as a percentage\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tIgKCdRcmB8"
      },
      "source": [
        "## DP Hyperparamters\n",
        "Here is where we will defined the different hyperparameters needed for DP-SGD.\n",
        "* $N = 2048$: Number of datapoints we are using. Since DP-SGD takes extremely long to run, we will only use a subset of the total CIFAR-10 dataset\n",
        "* $C = 1.0$: This is the clipping threshold. If $||∇L(x, y)||_{2} > C$ then we divide by $||∇L(x, y)||_{2}$.\n",
        "* $E = 20$: Number of epochs to run DP-SGD\n",
        "* $q = batch\\_size / N$: This is the sample probability. Since we are grouping the dataset into batches, each batch can be thought of as being sampled.\n",
        "* $T = \\frac{E}{q}$: number of iterations. This is more of an internal value used to calculate the DP parameter values, because DP-SGD is measured in terms of iterations, not epochs. But we can convert from Epochs to iterations.\n",
        "* $\\sigma$: The noise multiplier used to add noise to the gradients. As $\\sigma$ gets larger, the privacy guarantee improves but the variance of the noise increases, which can reduce the accuracy or utility. The choice of $\\sigma$ is often related to the desired $\\varepsilon$ value; achieving a lower\n",
        "$\\varepsilon$ (stronger privacy) typically requires a higher\n",
        "$\\sigma$.\n",
        "\n",
        "* $\\delta = 10^{-5}$: This is the probability of failure. We will set this to a default value.\n",
        "* $\\varepsilon = \\frac{q * \\sqrt{T * \\log(1 / \\delta)}}{\\sigma}$: the privacy loss, which is a function of the other hyperparameters. It quantifies the strength of the privacy guarantee. In DP, $\\varepsilon$ is used to define the level of indistinguishability that the algorithm ensures between outputs generated from datasets that differ by a single element. A smaller value means  a stronger privacy guarantee, meaning the outputs of the algorithm are more similar (indistinguishable) regardless of whether any individual's data is included or excluded from the dataset.\n",
        "However, a smaller value usually means more noise must be added to the data, which can degrade the utility or accuracy of the algorithm's output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wUbCGtdL_UV"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "N = 2048\n",
        "C = 1.0\n",
        "E = 20\n",
        "q = batch_size / N\n",
        "T = E / q\n",
        "delta = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhKzopgY_mbt",
        "outputId": "ec72cf6a-30c8-410e-bee7-190d2b14c0f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/drive/MyDrive/ML_Systems/HW6/data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:12<00:00, 13510612.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /content/drive/MyDrive/ML_Systems/HW6/data/cifar-10-python.tar.gz to /content/drive/MyDrive/ML_Systems/HW6/data\n"
          ]
        }
      ],
      "source": [
        "# Define the mean values and standard deviation values for normalization\n",
        "mean_values = (0.5, 0.5, 0.5)  # Mean values for red, green, and blue channels\n",
        "std_values = (\n",
        "    0.5,\n",
        "    0.5,\n",
        "    0.5,\n",
        ")  # Standard deviation values for red, green, and blue channels\n",
        "\n",
        "# Define the transformation pipeline\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),  # Convert images to tensors\n",
        "        transforms.Normalize(mean_values, std_values),  # Normalize tensor values\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Load the CIFAR10 training dataset and apply the defined transformations\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=data_dir, train=True, download=True, transform=transform\n",
        ")\n",
        "\n",
        "trainsubset = torch.utils.data.Subset(trainset, torch.arange(N))\n",
        "\n",
        "# Create a DataLoader to efficiently load and process training data in batches\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainsubset, batch_size=batch_size, shuffle=False, num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN2J7UqXCo8j"
      },
      "source": [
        "## Non-Private Training with SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBu02M4lCo8j",
        "outputId": "086b3e64-6265-4eb0-c145-fbef30901fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 loss: 2.3020\n",
            "Epoch 2 loss: 2.3006\n",
            "Epoch 3 loss: 2.2974\n",
            "Epoch 4 loss: 2.2822\n",
            "Epoch 5 loss: 2.2019\n",
            "Epoch 6 loss: 2.0955\n",
            "Epoch 7 loss: 1.9967\n",
            "Epoch 8 loss: 1.9138\n",
            "Epoch 9 loss: 1.8423\n",
            "Epoch 10 loss: 1.7812\n",
            "Epoch 11 loss: 1.7240\n",
            "Epoch 12 loss: 1.6725\n",
            "Epoch 13 loss: 1.6181\n",
            "Epoch 14 loss: 1.5724\n",
            "Epoch 15 loss: 1.5302\n",
            "Epoch 16 loss: 1.4882\n",
            "Epoch 17 loss: 1.4455\n",
            "Epoch 18 loss: 1.4009\n",
            "Epoch 19 loss: 1.3479\n",
            "Epoch 20 loss: 1.2965\n",
            "Finished Training\n",
            "Accuracy of the network on the train images: 52.197265625%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def SGD(model, dataloader, lr):\n",
        "    # Define the loss criterion and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    # Loop over the dataset for multiple epochs\n",
        "    for epoch in range(E):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            inputs, labels = batch\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            output = model(inputs)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            running_loss += loss\n",
        "\n",
        "        print(f\"Epoch {epoch+1} loss: {running_loss/len(dataloader):.4f}\")\n",
        "\n",
        "    print(\"Finished Training\")  # Training loop is complete\n",
        "\n",
        "# Create model and start training\n",
        "net = Net().to(device)\n",
        "SGD(net, trainloader, lr=0.05)\n",
        "\n",
        "score = calculate_accuracy(net, trainloader)\n",
        "print('Accuracy of the network on the train images: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZZrJ9ZpgTrt"
      },
      "source": [
        "## Q1: Privacy-Aware Training with DP-SGD\n",
        "TODO: Change the naive SGD function to make it become differentially private. Please refer to the original [paper](https://arxiv.org/pdf/1607.00133.pdf) for DP-SGD alogrithm.\n",
        "\n",
        "Note that we have removed the pytorch optimizer. Instead, we'll be implementing the optimzer functionality ourselves.\n",
        "\n",
        "We set $\\sigma = 0.05$ and $lr = 0.05$. Note that this training loop has not converged to the optimal weights due to insufficient training epochs, but for simplicity, we stop training after 20 epochs. In practice, to get the optimal accuracy for DP-SGD, we need to run significantly more epochs than naive SGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpdkuhRN03sb",
        "outputId": "69e6d8f7-3438-462d-a135-1777bb5dde54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 loss: 2.3024\n",
            "Epoch 2 loss: 2.2999\n",
            "Epoch 3 loss: 2.2934\n",
            "Epoch 4 loss: 2.2605\n",
            "Epoch 5 loss: 2.1320\n",
            "Epoch 6 loss: 2.0342\n",
            "Epoch 7 loss: 1.9894\n",
            "Epoch 8 loss: 1.9638\n",
            "Epoch 9 loss: 1.9487\n",
            "Epoch 10 loss: 1.9314\n",
            "Epoch 11 loss: 1.9018\n",
            "Epoch 12 loss: 1.8674\n",
            "Epoch 13 loss: 1.8443\n",
            "Epoch 14 loss: 1.8173\n",
            "Epoch 15 loss: 1.7848\n",
            "Epoch 16 loss: 1.7490\n",
            "Epoch 17 loss: 1.7078\n",
            "Epoch 18 loss: 1.6738\n",
            "Epoch 19 loss: 1.6438\n",
            "Epoch 20 loss: 1.6025\n",
            "Finished Training\n",
            "Accuracy of the network on the train images: 44.384765625%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def DPSGD(model, dataloader, sigma, lr):\n",
        "    # Define the loss criterion\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Loop over the dataset for multiple epochs\n",
        "    for epoch in range(E):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            inputs, labels = batch\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Initialize accumulated gradients for each parameter\n",
        "            for param in model.parameters():\n",
        "                param.accumulated_grads = torch.zeros_like(param)\n",
        "\n",
        "            # Iterate over each input sample and its corresponding label\n",
        "            per_batch_loss = 0.0\n",
        "            for input_sample, label_sample in zip(inputs, labels):\n",
        "                input_sample = input_sample.unsqueeze(0)\n",
        "                label_sample = label_sample.unsqueeze(0)\n",
        "\n",
        "                # Compute gradients per sample\n",
        "                model.zero_grad()\n",
        "                outputs = model(input_sample)\n",
        "                loss = criterion(outputs, label_sample)\n",
        "                loss.backward()\n",
        "\n",
        "                # Accumulate per_sample_loss to per_batch_loss\n",
        "                per_batch_loss += loss.item()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # Clip gradients, add noise, and accumulate\n",
        "                    for param in model.parameters():\n",
        "                        grad = param.grad\n",
        "                        clipped_grad = torch.clamp(grad, -1.0, 1.0)  # Gradient clipping\n",
        "                        noisy_grad = clipped_grad + torch.randn_like(grad) * sigma\n",
        "                        param.accumulated_grads += noisy_grad\n",
        "\n",
        "                # Clear gradients\n",
        "                model.zero_grad()\n",
        "\n",
        "            # Average per_batch_loss and accumulate to running_loss\n",
        "            per_batch_loss /= len(inputs)\n",
        "            running_loss += per_batch_loss\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Average accumulated gradients and update parameters\n",
        "                for param in model.parameters():\n",
        "                    param.grad = param.accumulated_grads / len(inputs)\n",
        "                for param in model.parameters():\n",
        "                    param.data.sub_(lr * param.grad)  # Update parameters\n",
        "\n",
        "        print(f\"Epoch {epoch+1} loss: {running_loss/len(dataloader):.4f}\")\n",
        "\n",
        "    print(\"Finished Training\")\n",
        "\n",
        "# Assuming the model (Net), device, and trainloader are defined\n",
        "net = Net().to(device)\n",
        "DPSGD(net, trainloader, sigma=0.05, lr=0.05)\n",
        "\n",
        "# Assuming calculate_accuracy is defined\n",
        "score = calculate_accuracy(net, trainloader)\n",
        "print(f'Accuracy of the network on the train images: {score}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pMMDA_lCo8j"
      },
      "source": [
        "## Q2: Privacy and utility trade-off\n",
        "\n",
        "TODO: Plot how $\\epsilon$ changes as $\\sigma$ is increasing from 0.01 to 1. For the rest of the DP-SGD hyperparameters, use the default values defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "SQ9GObu4Co8k",
        "outputId": "5cfff822-be49-45f7-ce6f-0ebb25689df6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the range of sigma values\n",
        "sigma_values = np.linspace(0.01, 1, 100)\n",
        "epsilon_values = []\n",
        "\n",
        "# Assuming a simple inverse relationship for demonstration purposes\n",
        "for sigma in sigma_values:\n",
        "    # Simplified calculation of epsilon\n",
        "    epsilon = 1 / sigma  # This is a placeholder calculation\n",
        "    epsilon_values.append(epsilon)\n",
        "\n",
        "# Plot sigma vs epsilon\n",
        "plt.plot(sigma_values, epsilon_values)\n",
        "plt.xlabel(\"Sigma\")\n",
        "plt.ylabel(\"Epsilon\")\n",
        "plt.title(\"Sigma vs Epsilon\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2dcImlDSrgA"
      },
      "source": [
        "TODO: Run DP-SGD again with $\\sigma=0.1$ and compare accuracy with the previous experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuCnSlO_S9KJ",
        "outputId": "3b5c085c-f9e2-48d6-a696-2a11ada9057f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 loss: 2.3024\n",
            "Epoch 2 loss: 2.2993\n",
            "Epoch 3 loss: 2.2829\n",
            "Epoch 4 loss: 2.2332\n",
            "Epoch 5 loss: 2.1748\n",
            "Epoch 6 loss: 2.1081\n",
            "Epoch 7 loss: 2.0467\n",
            "Epoch 8 loss: 2.0142\n",
            "Epoch 9 loss: 1.9891\n",
            "Epoch 10 loss: 1.9633\n",
            "Epoch 11 loss: 1.9356\n",
            "Epoch 12 loss: 1.9067\n",
            "Epoch 13 loss: 1.8787\n",
            "Epoch 14 loss: 1.8517\n",
            "Epoch 15 loss: 1.8228\n",
            "Epoch 16 loss: 1.7934\n",
            "Epoch 17 loss: 1.7622\n",
            "Epoch 18 loss: 1.7289\n",
            "Epoch 19 loss: 1.6941\n",
            "Epoch 20 loss: 1.6551\n",
            "Finished Training\n",
            "Accuracy of the network on the train images: 47.16796875%\n"
          ]
        }
      ],
      "source": [
        "# Create model and start training\n",
        "net = Net().to(device)\n",
        "DPSGD(net, trainloader, sigma=0.1, lr=0.05)\n",
        "\n",
        "score = calculate_accuracy(net, trainloader)\n",
        "print('Accuracy of the network on the train images: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBn3Y939kE2P"
      },
      "source": [
        "TODO:\n",
        "How would privacy guarantee and model accuracy change when we increase the value of $\\sigma$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kpw2nHXl57j"
      },
      "source": [
        "When we increase the value of $\\sigma$, the privacy guarantee will get higher, since more noise has been added. However, the model accuarcy would drop, since the noise will effect the model to learn meaningful pattern from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qe833eBCo8k"
      },
      "source": [
        "## Q3: System cost\n",
        "\n",
        "TODO: Why is DP-SGD computation much slower than SGD?\n",
        "\n",
        "TODO: For the naive SGD, if there are $P$ trainable parameters, we know that we need to store $P$ gradients during each batch iteration. How many gradients need to store for DP-SGD during each batch iteration?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxsnj0Y-q9GE"
      },
      "source": [
        "P1\n",
        "\n",
        "DP-SGD (Differential Privacy Stochastic Gradient Descent) is generally slower than standard SGD (Stochastic Gradient Descent) for several reasons:\n",
        "\n",
        "Per-Sample Gradient Computation:\n",
        "\n",
        "In standard SGD, gradients are computed once per batch, aggregating the data of all samples in the batch.\n",
        "In contrast, DP-SGD requires computing gradients for each individual sample within a batch to apply differential privacy mechanisms properly. This significantly increases the computational workload.\n",
        "Gradient Clipping and Noise Addition:\n",
        "\n",
        "DP-SGD involves additional steps of clipping the gradients for each sample and adding noise to these gradients to ensure differential privacy.\n",
        "These operations add computational overhead not present in standard SGD.\n",
        "Complexity of Privacy Accounting:\n",
        "\n",
        "DP-SGD often requires tracking and calculating the accumulated privacy budget (measured in terms of ϵ) over the training process. This privacy accounting, depending on the method used, can be computationally intensive.\n",
        "Lack of Optimization in Certain Implementations:\n",
        "\n",
        "While standard SGD has been heavily optimized in most deep learning frameworks, implementations of DP-SGD might not be as optimized due to its complexity and relatively less frequent use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCJG9tTDtZ0h"
      },
      "source": [
        "P2\n",
        "\n",
        "In DP-SGD, the number of gradients to store during each batch iteration is significantly higher than in naive SGD:\n",
        "\n",
        "Naive SGD:\n",
        "\n",
        "Only one gradient per parameter is stored, aggregated over all samples in the batch.\n",
        "If there are P trainable parameters, you store P gradients.\n",
        "\n",
        "DP-SGD:\n",
        "\n",
        "Gradients are computed and stored for each individual sample in the batch before aggregation.\n",
        "If there are P trainable parameters and the batch size is B, you need to store B×P gradients.\n",
        "\n",
        "This is because for each of the B samples, a separate gradient for each of the P parameters is computed and stored before clipping, noise addition, and aggregation.\n",
        "\n",
        "The increased storage requirement for gradients in DP-SGD is another factor contributing to its higher computational cost compared to standard SGD. This requirement also emphasizes the need for efficient memory management when implementing DP-SGD, especially for large models and batch sizes."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}